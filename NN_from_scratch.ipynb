{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        # Weight inputs, add bias, then use the activation function\n",
    "        total = np.dot(self.weight, inputs) + self.bias\n",
    "        return sigmoid(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0,1]) # w1 = 0, w2 = 1\n",
    "bias = 4 # b = 4\n",
    "\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2,3]) # x1 = 2, x2 = 3\n",
    "\n",
    "print(n.feed_forward(x)) # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Neurons into a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    '''\n",
    "    A Neural Network with:\n",
    "        1. 2 Inputs\n",
    "        2. A hidden layer with 2 neurons (h1, h2)\n",
    "        3. An output layer with 1 neuron (o1)\n",
    "    Each Neuron has the same weight and bias:\n",
    "        w = [0,1]\n",
    "        b = 0\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0,1])\n",
    "        bias = 0\n",
    "\n",
    "        # Neurons\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        out_h1 = self.h1.feed_forward(x)\n",
    "        out_h2 = self.h2.feed_forward(x)\n",
    "\n",
    "        # The inputs for o1 are the outputs of h1 and h2\n",
    "        out_o1 = self.o1.feed_forward(np.array([out_h1, out_h2]))\n",
    "\n",
    "        return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "network = Neural_Network()\n",
    "x = np.array([2,3])\n",
    "print(network.feed_forward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following measurements:\n",
    "\n",
    "Name\tWeight (lb)\t Height (in)\t  Gender\n",
    "Alice\t   133\t        65\t            F\n",
    "Bob\t       160\t        72\t            M\n",
    "Charlie\t   152\t        70\t            M\n",
    "Diana\t   120\t        60\t            F\n",
    "\n",
    "let's train our model to predict someone's gender based on thier weight and height\n",
    "\n",
    "We will represent male with a 0 and female with a 1, we will also shift the data to make it easier to use.\n",
    "\n",
    "Name\tWeight (-135)\t Height (-66)\t  Gender\n",
    "Alice\t    -2\t             -1\t            1\n",
    "Bob\t        25\t              6\t            0\n",
    "Charlie\t    17\t              4\t            0\n",
    "Diana\t   -15\t             -6\t            1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss\n",
    "\n",
    "Before we train the model, we first need a way to quantify how \"good\" its doing so we can improve.\n",
    "\n",
    "We will use the Mean Squared Error (MSE) Loss:\n",
    "MSE=  1/n 1->n ∑(y[true] - y[pred])**2\n",
    "\n",
    "n is the number of samples, which is 4 (Alice, Bob, Charlie, Diana).\n",
    "y represents the variable being predicted, which is Gender.\n",
    "y[true] is the true value of the variable (the “correct answer”). For example, y[true] for Alice would be 1 (Female).\n",
    "y[pred] is the predicted value of the variable. It’s whatever our network outputs.\n",
    "\n",
    "(y[true] - y[pred])**2 is known as the squared error. Our loss function is simply taking the average over all squared errors (hence the name mean squared error). The better our predictions are, the lower our loss will be!\n",
    "\n",
    "Better predictions = Lower loss.\n",
    "Training a network = trying to minimize its loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exampke of loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true: np.array, y_pred: np.array):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_pred = np.array([0,0,0,0])\n",
    "y_true = np.array([1,0,0,1])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a clear goal: minimize the loss of the neural network. We know we can change the network’s weights and biases to influence its predictions, but how do we do so in a way that decreases loss?\n",
    "\n",
    "We’ll use an optimization algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss.\n",
    "\n",
    "Our training process will look like this:\n",
    "\n",
    "1. Choose one sample from our dataset. This is what makes it stochastic gradient descent - we only operate on one sample at a time.\n",
    "2. Calculate all the partial derivatives of loss with respect to weights or biases.\n",
    "3. Use the update equation to update each weight and bias.\n",
    "4. Go back to step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class Neural_Network:\n",
    "    '''\n",
    "    A Neural Network with:\n",
    "        1. 2 Inputs\n",
    "        2. A hidden layer with 2 neurons (h1, h2)\n",
    "        3. An output layer with 1 neuron (o1)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "        \n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "\n",
    "        return o1\n",
    "    \n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "            - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "            - all_y_trues is a numpy array with n elements.\n",
    "            Elements in all_y_trues correspond to those in data.\n",
    "        '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # No.of times to loop through the entire dataset\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # Feed Forward\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # Calculate partial derivatives\n",
    "                # Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_y_pred = -2 *(y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # Update weights and biases\n",
    "\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_y_pred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_y_pred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_y_pred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_y_pred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_y_pred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_y_pred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_y_pred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_y_pred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_y_pred * d_ypred_d_b3\n",
    "            \n",
    "            # Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = np.apply_along_axis(self.feed_forward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_pred)\n",
    "                print(\"Epoch: %d loss: %.3f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 0.174\n",
      "Epoch: 10 loss: 0.111\n",
      "Epoch: 20 loss: 0.087\n",
      "Epoch: 30 loss: 0.071\n",
      "Epoch: 40 loss: 0.060\n",
      "Epoch: 50 loss: 0.051\n",
      "Epoch: 60 loss: 0.044\n",
      "Epoch: 70 loss: 0.039\n",
      "Epoch: 80 loss: 0.034\n",
      "Epoch: 90 loss: 0.031\n",
      "Epoch: 100 loss: 0.028\n",
      "Epoch: 110 loss: 0.025\n",
      "Epoch: 120 loss: 0.023\n",
      "Epoch: 130 loss: 0.021\n",
      "Epoch: 140 loss: 0.020\n",
      "Epoch: 150 loss: 0.018\n",
      "Epoch: 160 loss: 0.017\n",
      "Epoch: 170 loss: 0.016\n",
      "Epoch: 180 loss: 0.015\n",
      "Epoch: 190 loss: 0.014\n",
      "Epoch: 200 loss: 0.014\n",
      "Epoch: 210 loss: 0.013\n",
      "Epoch: 220 loss: 0.012\n",
      "Epoch: 230 loss: 0.012\n",
      "Epoch: 240 loss: 0.011\n",
      "Epoch: 250 loss: 0.011\n",
      "Epoch: 260 loss: 0.010\n",
      "Epoch: 270 loss: 0.010\n",
      "Epoch: 280 loss: 0.009\n",
      "Epoch: 290 loss: 0.009\n",
      "Epoch: 300 loss: 0.009\n",
      "Epoch: 310 loss: 0.008\n",
      "Epoch: 320 loss: 0.008\n",
      "Epoch: 330 loss: 0.008\n",
      "Epoch: 340 loss: 0.008\n",
      "Epoch: 350 loss: 0.007\n",
      "Epoch: 360 loss: 0.007\n",
      "Epoch: 370 loss: 0.007\n",
      "Epoch: 380 loss: 0.007\n",
      "Epoch: 390 loss: 0.006\n",
      "Epoch: 400 loss: 0.006\n",
      "Epoch: 410 loss: 0.006\n",
      "Epoch: 420 loss: 0.006\n",
      "Epoch: 430 loss: 0.006\n",
      "Epoch: 440 loss: 0.006\n",
      "Epoch: 450 loss: 0.006\n",
      "Epoch: 460 loss: 0.005\n",
      "Epoch: 470 loss: 0.005\n",
      "Epoch: 480 loss: 0.005\n",
      "Epoch: 490 loss: 0.005\n",
      "Epoch: 500 loss: 0.005\n",
      "Epoch: 510 loss: 0.005\n",
      "Epoch: 520 loss: 0.005\n",
      "Epoch: 530 loss: 0.005\n",
      "Epoch: 540 loss: 0.005\n",
      "Epoch: 550 loss: 0.004\n",
      "Epoch: 560 loss: 0.004\n",
      "Epoch: 570 loss: 0.004\n",
      "Epoch: 580 loss: 0.004\n",
      "Epoch: 590 loss: 0.004\n",
      "Epoch: 600 loss: 0.004\n",
      "Epoch: 610 loss: 0.004\n",
      "Epoch: 620 loss: 0.004\n",
      "Epoch: 630 loss: 0.004\n",
      "Epoch: 640 loss: 0.004\n",
      "Epoch: 650 loss: 0.004\n",
      "Epoch: 660 loss: 0.004\n",
      "Epoch: 670 loss: 0.004\n",
      "Epoch: 680 loss: 0.004\n",
      "Epoch: 690 loss: 0.003\n",
      "Epoch: 700 loss: 0.003\n",
      "Epoch: 710 loss: 0.003\n",
      "Epoch: 720 loss: 0.003\n",
      "Epoch: 730 loss: 0.003\n",
      "Epoch: 740 loss: 0.003\n",
      "Epoch: 750 loss: 0.003\n",
      "Epoch: 760 loss: 0.003\n",
      "Epoch: 770 loss: 0.003\n",
      "Epoch: 780 loss: 0.003\n",
      "Epoch: 790 loss: 0.003\n",
      "Epoch: 800 loss: 0.003\n",
      "Epoch: 810 loss: 0.003\n",
      "Epoch: 820 loss: 0.003\n",
      "Epoch: 830 loss: 0.003\n",
      "Epoch: 840 loss: 0.003\n",
      "Epoch: 850 loss: 0.003\n",
      "Epoch: 860 loss: 0.003\n",
      "Epoch: 870 loss: 0.003\n",
      "Epoch: 880 loss: 0.003\n",
      "Epoch: 890 loss: 0.003\n",
      "Epoch: 900 loss: 0.003\n",
      "Epoch: 910 loss: 0.003\n",
      "Epoch: 920 loss: 0.003\n",
      "Epoch: 930 loss: 0.002\n",
      "Epoch: 940 loss: 0.002\n",
      "Epoch: 950 loss: 0.002\n",
      "Epoch: 960 loss: 0.002\n",
      "Epoch: 970 loss: 0.002\n",
      "Epoch: 980 loss: 0.002\n",
      "Epoch: 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2,-1], # Alice\n",
    "    [25, 6], # Bob\n",
    "    [17, 4], # Charlie\n",
    "    [-15,-6], # Diana\n",
    "])\n",
    "\n",
    "all_y_trues = np.array([\n",
    "    1, # Alice\n",
    "    0, # Bob\n",
    "    0, # Charlie\n",
    "    1, # Diana\n",
    "])\n",
    "\n",
    "# Train our Neural Network\n",
    "network = Neural_Network()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.948\n",
      "Frank: 0.039\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feed_forward(emily)) # 0.948 - F\n",
    "print(\"Frank: %.3f\" % network.feed_forward(frank)) # 0.039 - M\n",
    "\n",
    "# epochs 1000     10,000 \n",
    "# Emily: 0.948     0.990\n",
    "# Frank: 0.039     0.016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
